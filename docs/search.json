[
  {
    "objectID": "projects/entity_extraction/index.html",
    "href": "projects/entity_extraction/index.html",
    "title": "Entity extraction from retail images",
    "section": "",
    "text": "Source Code  Competition page\nThis was a 3 day hack-a-thon organised by Amazon India as part of their annual Amazon ML Challenge, an inter-college team competition."
  },
  {
    "objectID": "projects/entity_extraction/index.html#problem-statement",
    "href": "projects/entity_extraction/index.html#problem-statement",
    "title": "Entity extraction from retail images",
    "section": "Problem Statement",
    "text": "Problem Statement\nIn the hackathon, the goal was to create a machine learning model that extracts entity values from images. As digital marketplaces expand, many products lack detailed textual descriptions, making it essential to obtain key details directly from images. These images provide important information such as weight, volume, voltage, wattage, dimensions, and many more, which are critical for digital stores."
  },
  {
    "objectID": "projects/entity_extraction/index.html#dataset",
    "href": "projects/entity_extraction/index.html#dataset",
    "title": "Entity extraction from retail images",
    "section": "Dataset",
    "text": "Dataset\nThe dataset consists of the following columns:\n\nindex: An unique identifier (ID) for the data sample\nimage_link: Public URL where the product image is available for download. Example link. To download images use download_images function from src/utils.py. See sample code in src/test.ipynb.\ngroup_id: Category code of the product\nentity_name: Product entity name. For eg: “item_weight”\nentity_value: Product entity value. For eg: “34 gram”. Note: For test.csv, you will not see the column entity_value as it is the target variable."
  },
  {
    "objectID": "projects/entity_extraction/index.html#file-description",
    "href": "projects/entity_extraction/index.html#file-description",
    "title": "Entity extraction from retail images",
    "section": "File description",
    "text": "File description\nthe src and dataset folders are provided by the organisers\n\nsource files\n\nsrc/sanity.py: Sanity checker to ensure that the final output file passes all formatting checks. Note: the script will not check if less/more number of predictions are present compared to the test file. See sample code in src/test.ipynb\nsrc/utils.py: Contains helper functions for downloading images from the image_link.\nsrc/constants.py: Contains the allowed units for each entity type.\nsrc/sample_code.py: We also provided a sample dummy code that can generate an output file in the given format. Usage of this file is optional.\n\n\n\ndataset files\n\ndataset/train.csv: Training file with labels (entity_value).\ndataset/test.csv: Test file without output labels (entity_value). Generate predictions using your model/solution on this file’s data and format the output file to match sample_test_out.csv (Refer the above section “Output Format”)\ndataset/sample_test.csv: Sample test input file.\ndataset/sample_test_out.csv: Sample outputs for sample_test.csv. The output for test.csv must be formatted in the exact same way. Note: The predictions in the file might not be correct\n\n\n\nsolution file\n\nocr_regex.ipynb: Solution notebook detailing our approach."
  },
  {
    "objectID": "projects/entity_extraction/index.html#evaluation-criteria",
    "href": "projects/entity_extraction/index.html#evaluation-criteria",
    "title": "Entity extraction from retail images",
    "section": "Evaluation Criteria",
    "text": "Evaluation Criteria\nSubmissions will be evaluated based on F1 score\nLet GT = Ground truth value for a sample and OUT be output prediction from the model for a sample. Then we classify the predictions into one of the 4 classes with the following logic:\n\nTrue Positives - If OUT != “” and GT != “” and OUT == GT\nFalse Positives - If OUT != “” and GT != “” and OUT != GT\nFalse Positives - If OUT != “” and GT == “”\nFalse Negatives - If OUT == “” and GT != “”\nTrue Negatives - If OUT == “” and GT == “”\n\nThen, F1 score = 2(Precision)(Recall)/(Precision + Recall) where: - Precision = True Positives/(True Positives + False Positives) - Recall = True Positives/(True Positives + False Negatives)"
  },
  {
    "objectID": "projects/entity_extraction/index.html#approach",
    "href": "projects/entity_extraction/index.html#approach",
    "title": "Entity extraction from retail images",
    "section": "Approach",
    "text": "Approach\n\nDesigning the OCR Pipeline\nI created a robust OCR pipeline to automate key steps in image processing and text extraction. This included tasks such as image loading, grayscaling, and binarization to enhance text visibility and clarity. These preprocessed images were then passed through EasyOCR, to accurately extract text.\n\n\nEntity Parsing and Extraction\nTo transform raw OCR output into actionable data, I utilized regular expression matching. This approach allowed me to efficiently parse the extracted text and identify specific entities of interest. By tailoring the parsing logic to the dataset’s structure, I ensured reliable extraction of key information."
  },
  {
    "objectID": "projects/entity_extraction/index.html#result",
    "href": "projects/entity_extraction/index.html#result",
    "title": "Entity extraction from retail images",
    "section": "Result",
    "text": "Result\nThe outlined approach earned us a spot in the top 500 rankings on a highly competitive national leaderboard, which featured over 75,000 participants."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blogs",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I am a Deep Learning practitioner, currently working as a Data Scientist at Turing. I am based in Thane, Maharashtra. I am an avid fantasy fiction reader and I like playing chess. I have been practicing deep learning for almost two years now and I am excited to grow in this field!\nI am a 2025 graduate from Indian Institute of Technology (BHU), Varanasi, where I completed my B.Tech in Mechanical Engineering. This is where I first got into machine learning and deep learning, starting off with Andrew Ng’s Machine Learning Specialization course on Coursera, and eventually doing the fast.ai course: Practical Deep Learning for Coders.\nWhile at IIT Varanasi, I did a summer internship as a Data Scientist at Voiceback Analytics, where I led promotion analytics for Mumbai International Airport’s duty-free stores. I developed a pipeline to extract transactional data and trained machine learning models to identify the most effective promotions. Apart from the internship, I did various projects to cement my practical understanding. Check out some of my notable side projects here.\nI am always open to connecting and collaborating, feel free to reach out:\n E-mail  Github  Linkedin  Twitter"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I am Utkarsh!",
    "section": "",
    "text": "Currently participating in the CMI - Detect Behavior with Sensor Data kaggle code competition\n\n\n\n\n\n\n\nProjects\nSome of my more notable side projects\n\n\n\n\n\n\n\n\n\n\nEntity extraction from retail images\n\n\n\nOptical Character Recognition\n\nRegex Matching\n\n\n\nApplied machine learning models to extract precise product information directly from retail images\n\n\n\n\n\n\n\n\n\n\n\n\nSkin Cancer Detection with 3D-TBP\n\n\n\nImage Classification\n\nMultimodal\n\nEnsembling\n\n\n\nIdentified cancers among skin lesions cropped from 3D total body photographs along with metadata\n\n\n\n\n\nNo matching items\n\n\n\nLet’s get in touch!\nI am always open to connecting and collaborating, feel free to reach out.\n E-mail  Github  Linkedin  Twitter\n\nTo know more about me and my journey, visit the about page."
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html",
    "href": "projects/skin_cancer_detection/index.html",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "",
    "text": "Source Code  Competition page\nThe International Skin Imaging Collaboration (ISIC) is an international academia and industry partnership designed to reduce skin cancer morbidity and mortality through the development and use of digital skin imaging applications. Beginning in 2016, ISIC has sponsored Grand Challenges for the computer science community in association with leading computer vision conferences. Over the years, these challenges have grown in scale, complexity, and participation."
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html#problem-statement",
    "href": "projects/skin_cancer_detection/index.html#problem-statement",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe task is to develop image-based algorithms to identify histologically confirmed skin cancer cases with single-lesion crops from 3D total body photos (TBP). The image quality resembles close-up smartphone photos, which are regularly submitted for telehealth purposes. The binary classification algorithm could be used in settings without access to specialized care and improve triage for early skin cancer detection."
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html#data",
    "href": "projects/skin_cancer_detection/index.html#data",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "Data",
    "text": "Data\nThe dataset consists of diagnostically labelled images with additional metadata. The images are JPEGs. The associated .csv file contains a binary diagnostic label (target), potential input variables (e.g. age_approx, sex, anatom_site_general, etc.), and additional attributes (e.g. image source and precise diagnosis). In the challenge, one is to differentiate benign from malignant cases. For each image (isic_id) one is to assign the probability (target) ranging [0, 1] that the case is malignant. To download the data files, please run the following shell command:\nkaggle competitions download -c isic-2024-challenge"
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html#evaluation",
    "href": "projects/skin_cancer_detection/index.html#evaluation",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "Evaluation",
    "text": "Evaluation\nSubmissions are evaluated on partial area under the ROC curve (pAUC) above 80% true positive rate (TPR) for binary classification of malignant examples. The receiver operating characteristic (ROC) curve illustrates the diagnostic ability of a given binary classifier system as its discrimination threshold is varied. However, there are regions in the ROC space where the values of TPR are unacceptable in clinical practice. Systems that aid in diagnosing cancers are required to be highly-sensitive, so this metric focuses on the area under the ROC curve AND above 80% TRP. Hence, scores range from [0.0, 0.2]."
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html#approach",
    "href": "projects/skin_cancer_detection/index.html#approach",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "Approach",
    "text": "Approach\nI adopted a structured and multi-layered methodology to achieve significant performance improvements.\n\nData Pre-processing\nThe task involved tackling an imbalanced binary classification problem, where the training dataset had a class ratio of 1:1000 (positive to negative). To address this, I first undersampled the negative class and then oversampled the positive class, achieving a balanced ratio of 2:3. Additionally, I incorporated penalties in the loss function to account for the adjusted class. To enhance the dataset, I applied various image augmentation techniques and relied extensively on the fastai library for efficient data preprocessing and augmentation workflows.\n\n\nFine-tuning the Base Model\nI fine-tuned a pre-trained ResNet-18 model on the single lesion crops. I utilized StratifiedGroupKFold to ensure robust evaluation and also to generate Out-Of-Fold (OOF) predictions.\n\n\nBuilding an Ensemble System\nLeveraging the OOF predictions from the ResNet-18 image model, I integrated additional predictive layers by stacking three powerful gradient boosting models: XGBoost, CatBoost, and LightGBM. Each of these models independently processed the image features, and their outputs were combined using a soft voting ensemble. This ensemble strategy balanced the strengths of individual models, yielding a more generalized and accurate prediction system."
  },
  {
    "objectID": "projects/skin_cancer_detection/index.html#result",
    "href": "projects/skin_cancer_detection/index.html#result",
    "title": "Skin Cancer Detection with 3D-TBP",
    "section": "Result",
    "text": "Result\nThe ensemble approach resulted in a significant improvement in the partial AUC-ROC score. The metric increased to 0.167, outperforming the standalone image model’s score of 0.144, thus showcasing the enhanced predictive capability of the multi-model ensemble."
  }
]